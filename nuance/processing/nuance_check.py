import asyncio
import time
from typing import Any, ClassVar, Dict
import traceback

import aiohttp

import nuance.constants as cst
import nuance.models as models
from nuance.utils.logging import logger
from nuance.utils.networking import async_http_request_with_retry
from nuance.processing.base import Processor, ProcessingResult
from nuance.processing.llm import query_llm
from nuance.constitution import constitution_store


class NuanceChecker(Processor):
    """Checks content for nuanced thinking using LLM."""
    processor_name = "nuance_checker"
    
    # # Class-level cache for nuance prompt only
    # _nuance_prompt_cache: ClassVar[Dict[str, Any]] = {"prompt": None, "last_updated": None}
    # _nuance_prompt_lock = asyncio.Lock()
    
    # async def get_nuance_prompt(self) -> str:
    #     """
    #     Get nuance evaluation prompt with caching.
    #     Checks for updated prompt based on configured interval.
    #     """
    #     current_time = time.time()
        
    #     # Check if update is needed without acquiring the lock
    #     if (
    #         self._nuance_prompt_cache["last_updated"] is None
    #         or current_time - self._nuance_prompt_cache["last_updated"]
    #         > cst.NUANCE_CONSTITUTION_UPDATE_INTERVAL
    #     ):
    #         # Only acquire the lock if update might be needed
    #         async with self._nuance_prompt_lock:
    #             # Re-check after acquiring the lock (another task might have updated meanwhile)
    #             if (
    #                 self._nuance_prompt_cache["last_updated"] is None
    #                 or current_time - self._nuance_prompt_cache["last_updated"]
    #                 > cst.NUANCE_CONSTITUTION_UPDATE_INTERVAL
    #             ):
    #                 # Update the cache if it's older than the update interval
    #                 try:
    #                     post_evaluation_prompt_url = (
    #                         cst.NUANCE_CONSTITUTION_STORE_URL + "post_evaluation_prompt.txt"
    #                     )

    #                     async with aiohttp.ClientSession() as session:
    #                         # Only fetch the nuance evaluation prompt
    #                         post_evaluation_prompt = await async_http_request_with_retry(
    #                             session, "GET", post_evaluation_prompt_url
    #                         )
                            
    #                     # Store only the nuance prompt in cache
    #                     self._nuance_prompt_cache["prompt"] = post_evaluation_prompt
    #                     self._nuance_prompt_cache["last_updated"] = current_time
    #                     logger.info("‚úÖ Nuance evaluation prompt updated successfully")
    #                 except Exception as e:
    #                     logger.error(f"‚ùå Error fetching nuance prompt: {traceback.format_exc()}")

    #     return self._nuance_prompt_cache["prompt"]
    
    async def process(self, input_data: models.Post) -> ProcessingResult[models.Post]:
        """
        Check if content shows nuanced thinking.
        
        Args:
            post: Post object to check
            
        Returns:
            Processing result with nuance check status
        """
        try:
            post = input_data
            post_id = post.post_id
            content = post.content
            
            # Get the nuance prompt
            nuance_prompt = await constitution_store.get_nuance_prompt()
            
            # Format the prompt with the post content
            prompt_nuance = nuance_prompt.format(tweet_text=content)
            
            # Call LLM to evaluate nuance
            llm_response = await query_llm(prompt=prompt_nuance, temperature=0.0)
            
            # Check if the post is approved as nuanced
            is_nuanced = llm_response.strip().lower() == "approve"
            
            if is_nuanced:
                logger.info(f"‚úÖ Post {post_id} is nuanced")
                return ProcessingResult(
                    status=models.ProcessingStatus.ACCEPTED,
                    output=post, 
                    processor_name=self.processor_name,
                    details={"nuance_status": "approved", "llm_response": llm_response}
                )
            else:
                logger.info(f"üö´ Post {post_id} is not nuanced")
                return ProcessingResult(
                    status=models.ProcessingStatus.REJECTED,
                    output=post, 
                    processor_name=self.processor_name,
                    reason="Content lacks nuance",
                    details={"llm_response": llm_response}
                )
                
        except Exception as e:
            logger.error(f"‚ùå Error evaluating nuance for post {post.post_id}: {str(e)}")
            return ProcessingResult(
                status=models.ProcessingStatus.ERROR,
                output=post, 
                processor_name=self.processor_name,
                reason=f"Error checking nuance: {str(e)}"
            )